{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      created_at               id_str  \\\n",
      "0      2023-12-05 00:01:01+07:00  1731720272468689309   \n",
      "1      2023-12-05 00:05:15+07:00  1731721336773615908   \n",
      "2      2023-12-05 00:06:05+07:00  1731721547528745220   \n",
      "3      2023-12-05 00:08:59+07:00  1731722276154814877   \n",
      "4      2023-12-05 00:10:35+07:00  1731722678409830561   \n",
      "...                          ...                  ...   \n",
      "13710  2023-12-29 23:47:01+07:00  1740776447801983331   \n",
      "13711  2023-12-29 23:49:51+07:00  1740777158916940199   \n",
      "13712  2023-12-29 23:51:28+07:00  1740777564119945340   \n",
      "13713  2023-12-29 23:51:40+07:00  1740777616150560856   \n",
      "13714  2023-12-29 23:58:00+07:00  1740779211957383379   \n",
      "\n",
      "                                               full_text  quote_count  \\\n",
      "0      For Hindu girls who marry Muslims hijab is jus...            1   \n",
      "1      Cak  Indonesia bukan Stadion. Bangun stadion b...            0   \n",
      "2                           Amin https://t.co/mpajRO6X4g            0   \n",
      "3      @Alia20268374 @connorboutit @billardthegoat @M...            1   \n",
      "4      Selamat datang ( Krue Semangat) Gus Muhaimin @...            0   \n",
      "...                                                  ...          ...   \n",
      "13710  üöÄN–ïW CRAZY PUMP PROJECTüöÄ https://t.co/2pvfA8nR...            0   \n",
      "13711  Allahuakbar üò≠ ya Allah please have mercy on th...            0   \n",
      "13712  @TheRubberDuck79 How many have actually looked...            0   \n",
      "13713                                 pak anies please üôè            0   \n",
      "13714          I‚Äôm joining this anies x ganjar bandwagon            0   \n",
      "\n",
      "       reply_count  retweet_count  favorite_count lang          user_id_str  \\\n",
      "0                1              4              16   en            823691984   \n",
      "1                0              0               0   in           1179824329   \n",
      "2                1              0               6   en  1455113816262512643   \n",
      "3                9              1               1   en  1672412034883805184   \n",
      "4                0              0               0   in   949664160295542784   \n",
      "...            ...            ...             ...  ...                  ...   \n",
      "13710            0              0               0   en  1733448950881878016   \n",
      "13711            0              2               0   en            619824668   \n",
      "13712           11              6             140   en           4871480900   \n",
      "13713            0              0               0   en  1389901935759613955   \n",
      "13714            1              2              18   en           2904379412   \n",
      "\n",
      "       conversation_id_str         username  \\\n",
      "0      1731720272468689309     PoojaPophale   \n",
      "1      1731721336773615908      juarajuntak   \n",
      "2      1731721547528745220      zubizubi_du   \n",
      "3      1731294519868461278       UnLovedGoy   \n",
      "4      1731722678409830561   Syahrullah_hsb   \n",
      "...                    ...              ...   \n",
      "13710  1740776447801983331  sundmobout83558   \n",
      "13711  1740777158916940199         ainabas_   \n",
      "13712  1740771017826689112      CindyBublik   \n",
      "13713  1740777616150560856        pesisilan   \n",
      "13714  1740779211957383379        Crisctina   \n",
      "\n",
      "                                               tweet_url  \\\n",
      "0      https://twitter.com/PoojaPophale/status/173172...   \n",
      "1      https://twitter.com/juarajuntak/status/1731721...   \n",
      "2      https://twitter.com/zubizubi_du/status/1731721...   \n",
      "3      https://twitter.com/UnLovedGoy/status/17317222...   \n",
      "4      https://twitter.com/Syahrullah_hsb/status/1731...   \n",
      "...                                                  ...   \n",
      "13710  https://twitter.com/sundmobout83558/status/174...   \n",
      "13711  https://twitter.com/ainabas_/status/1740777158...   \n",
      "13712  https://twitter.com/CindyBublik/status/1740777...   \n",
      "13713  https://twitter.com/pesisilan/status/174077761...   \n",
      "13714  https://twitter.com/Crisctina/status/174077921...   \n",
      "\n",
      "                                             image_url  \\\n",
      "0                                                  NaN   \n",
      "1                                                  NaN   \n",
      "2      https://pbs.twimg.com/media/GAhPkIDWIAEpLw0.jpg   \n",
      "3                                                  NaN   \n",
      "4                                                  NaN   \n",
      "...                                                ...   \n",
      "13710                                              NaN   \n",
      "13711                                              NaN   \n",
      "13712                                              NaN   \n",
      "13713                                              NaN   \n",
      "13714                                              NaN   \n",
      "\n",
      "                            location  keyword  \n",
      "0                                NaN  Akronim  \n",
      "1                          Tangerang   Wapres  \n",
      "2                                NaN  Akronim  \n",
      "3      Hashemite Kingdom of Macrobia  Akronim  \n",
      "4      Pidie, Nangro Aceh Darussalam   Wapres  \n",
      "...                              ...      ...  \n",
      "13710                            NaN  Akronim  \n",
      "13711      Kuala Lumpur, Malaysia üá≤üáæ  Akronim  \n",
      "13712                  United States  Akronim  \n",
      "13713                     she/her 22     Pres  \n",
      "13714                            NaN     Pres  \n",
      "\n",
      "[13715 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing your datasets\n",
    "data_directory = \"cleaned_dataset/paslon1/\"\n",
    "\n",
    "# List to store individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through files in the directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith(\".csv\"):  # Assuming your datasets are in CSV format\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        \n",
    "        # Read each CSV file into a DataFrame and append it to the list\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one big DataFrame\n",
    "big_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(big_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Data by Text:  135\n",
      "Total Data after Drop:  13580\n"
     ]
    }
   ],
   "source": [
    "# count duplicate data based on full_text\n",
    "print(\"Duplicate Data by Text: \", big_df['full_text'].duplicated().sum())\n",
    "\n",
    "# drop duplicate data based on full_text\n",
    "big_df = big_df.drop_duplicates(subset=['full_text'])\n",
    "print(\"Total Data after Drop: \", len(big_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "big_df['created_at'] = pd.to_datetime(big_df['your_datetime_column'], utc=True).dt.tz_convert('Asia/Bangkok')\n",
    "\n",
    "big_df.to_csv('combined_dataset/Paslon1_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7500 entries, 1065 to 901\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   created_at           7500 non-null   object\n",
      " 1   id_str               7500 non-null   int64 \n",
      " 2   full_text            7500 non-null   object\n",
      " 3   quote_count          7500 non-null   int64 \n",
      " 4   reply_count          7500 non-null   int64 \n",
      " 5   retweet_count        7500 non-null   int64 \n",
      " 6   favorite_count       7500 non-null   int64 \n",
      " 7   lang                 7500 non-null   object\n",
      " 8   user_id_str          7500 non-null   int64 \n",
      " 9   conversation_id_str  7500 non-null   int64 \n",
      " 10  username             7500 non-null   object\n",
      " 11  tweet_url            7500 non-null   object\n",
      " 12  image_url            3889 non-null   object\n",
      " 13  location             4052 non-null   object\n",
      " 14  keyword              7500 non-null   object\n",
      "dtypes: int64(7), object(8)\n",
      "memory usage: 937.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# take random sample data\n",
    "df_sample = big_df.sample(n=7500, random_state=42)\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yusuf\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2059\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2059\u001b[0m     values, tz_parsed \u001b[39m=\u001b[39m conversion\u001b[39m.\u001b[39;49mdatetime_to_datetime64(data)\n\u001b[0;32m   2060\u001b[0m     \u001b[39m# If tzaware, these values represent unix timestamps, so we\u001b[39;00m\n\u001b[0;32m   2061\u001b[0m     \u001b[39m#  return them as i8 to distinguish from wall times\u001b[39;00m\n",
      "File \u001b[1;32mpandas\\_libs\\tslibs\\conversion.pyx:335\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Array must be all same time zone",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# take date from created_at\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yusuf\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:803\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m    801\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39mmap(cache_array)\n\u001b[0;32m    802\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 803\u001b[0m         values \u001b[39m=\u001b[39m convert_listlike(arg\u001b[39m.\u001b[39;49m_values, \u001b[39mformat\u001b[39;49m)\n\u001b[0;32m    804\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39m_constructor(values, index\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mindex, name\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mname)\n\u001b[0;32m    805\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[39m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32mc:\\Users\\yusuf\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:459\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m infer_datetime_format\n\u001b[0;32m    458\u001b[0m     utc \u001b[39m=\u001b[39m tz \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutc\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 459\u001b[0m     result, tz_parsed \u001b[39m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    460\u001b[0m         arg,\n\u001b[0;32m    461\u001b[0m         dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[0;32m    462\u001b[0m         yearfirst\u001b[39m=\u001b[39;49myearfirst,\n\u001b[0;32m    463\u001b[0m         utc\u001b[39m=\u001b[39;49mutc,\n\u001b[0;32m    464\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    465\u001b[0m         require_iso8601\u001b[39m=\u001b[39;49mrequire_iso8601,\n\u001b[0;32m    466\u001b[0m         allow_object\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    467\u001b[0m     )\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m tz_parsed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     \u001b[39m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[39m# is in UTC\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     dta \u001b[39m=\u001b[39m DatetimeArray(result, dtype\u001b[39m=\u001b[39mtz_to_dtype(tz_parsed))\n",
      "File \u001b[1;32mc:\\Users\\yusuf\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2064\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[0;32m   2062\u001b[0m         \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mview(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m), tz_parsed\n\u001b[0;32m   2063\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m-> 2064\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m tz_parsed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2067\u001b[0m     \u001b[39m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m     \u001b[39m#  is in UTC\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m     \u001b[39m# Return i8 values to denote unix timestamps\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mview(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m), tz_parsed\n",
      "File \u001b[1;32mc:\\Users\\yusuf\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2049\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[0;32m   2046\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mobject_)\n\u001b[0;32m   2048\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2049\u001b[0m     result, tz_parsed \u001b[39m=\u001b[39m tslib\u001b[39m.\u001b[39;49marray_to_datetime(\n\u001b[0;32m   2050\u001b[0m         data,\n\u001b[0;32m   2051\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   2052\u001b[0m         utc\u001b[39m=\u001b[39;49mutc,\n\u001b[0;32m   2053\u001b[0m         dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[0;32m   2054\u001b[0m         yearfirst\u001b[39m=\u001b[39;49myearfirst,\n\u001b[0;32m   2055\u001b[0m         require_iso8601\u001b[39m=\u001b[39;49mrequire_iso8601,\n\u001b[0;32m   2056\u001b[0m     )\n\u001b[0;32m   2057\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   2058\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mpandas\\_libs\\tslib.pyx:352\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\tslib.pyx:435\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True"
     ]
    }
   ],
   "source": [
    "# take date from created_at\n",
    "df_sample['your_datetime_column'] = pd.to_datetime(df_sample['your_datetime_column'], utc=True).dt.tz_convert('Asia/Bangkok')\n",
    "\n",
    "df_sample['created_at'] = pd.to_datetime(df_sample['created_at'])\n",
    "# df_sample['date'] = df_sample['created_at'].dt.date\n",
    "# df_sample['date'] = pd.to_datetime(df_sample['date'])\n",
    "# df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "df_sample.to_csv('combined_dataset/Paslon1_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e26da9777431fcd98778416b583e2cb3f0c4f5515b7897587b203af2bddddd45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
