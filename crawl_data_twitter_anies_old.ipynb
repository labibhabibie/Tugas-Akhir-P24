{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/labibhabibie/Tugas-Akhir-P24/blob/main/crawl_data_twitter_anies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAE7WPkQINAz"
      },
      "source": [
        "# Crawling Twitter Data based on **Search Keyword** Using `tweet-harvest`\n",
        "Last updated by Helmi Satria (helmisatria.com) on September 22, 2023\n",
        "\n",
        "Note: **For educational purposes only**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4UIL1x21P9rQ",
        "outputId": "efda98a8-dddc-4230-ecb6-8f1a8fdc7d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb7RNTzKfPtY"
      },
      "source": [
        "## Parameter Crawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yp0XaxzdU6k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb2ZoBCffZnk"
      },
      "outputs": [],
      "source": [
        "tanggal = '2023-12-12'\n",
        "token = \"8cf305a990ab7882de4452e9f68e022de96235a2\"\n",
        "limit = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjioCVy1akDi"
      },
      "source": [
        "## Anies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYDR51dJlVlX"
      },
      "outputs": [],
      "source": [
        "# Crawl Data Twitter\n",
        "paslon = 'Anies'\n",
        "filename = f'{paslon}_{tanggal}.csv'\n",
        "\n",
        "search_keyword = f'{paslon} since:{tanggal}_00:00:00_WIB until:{tanggal}_23:59:59_WIB'\n",
        "\n",
        "!npx --yes tweet-harvest@latest -o \"{filename}\" -s \"{search_keyword}\" -l {limit} --token {token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h9n7V_lZutL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# sort by created_at\n",
        "df = df.sort_values(by=['created_at'])\n",
        "\n",
        "# change datetime to Asia/Jakarta\n",
        "df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "df['created_at'] = df['created_at'].dt.tz_convert('Asia/Jakarta')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWnai8VxZ5mR"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NdHDQ9SaxCO"
      },
      "source": [
        "## Muhaimin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2gXBG09X_xt"
      },
      "outputs": [],
      "source": [
        "# Crawl Data Twitter\n",
        "paslon2 = 'Muhaimin'\n",
        "# tanggal = '2023-12-10'\n",
        "filename2 = f'{paslon2}_{tanggal}.csv'\n",
        "\n",
        "search_keyword2 = f'{paslon2} since:{tanggal}_00:00:00_WIB until:{tanggal}_23:59:59_WIB'\n",
        "# limit = 5000\n",
        "\n",
        "!npx --yes tweet-harvest@latest -o \"{filename2}\" -s \"{search_keyword2}\" -l {limit} --token {token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HvAG3hPvQDqk"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your CSV file\n",
        "file_path2 = f\"tweets-data/{filename2}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df2 = pd.read_csv(file_path2)\n",
        "\n",
        "# sort by created_at\n",
        "df2 = df2.sort_values(by=['created_at'])\n",
        "\n",
        "# change datetime to Asia/Jakarta\n",
        "df2['created_at'] = pd.to_datetime(df2['created_at'])\n",
        "df2['created_at'] = df2['created_at'].dt.tz_convert('Asia/Jakarta')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1numHF8QDiV"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWiM1SXMbL4O"
      },
      "source": [
        "## 3. Amin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oo1hxAHbXy_"
      },
      "outputs": [],
      "source": [
        "# Crawl Data Twitter\n",
        "paslon3 = 'Amin'\n",
        "# tanggal = '2023-12-10'\n",
        "filename3 = f'{paslon3}_{tanggal}.csv'\n",
        "\n",
        "search_keyword3 = f'{paslon3} since:{tanggal}_00:00:00_WIB until:{tanggal}_23:59:59_WIB'\n",
        "# limit = 5000\n",
        "\n",
        "!npx --yes tweet-harvest@latest -o \"{filename3}\" -s \"{search_keyword3}\" -l {limit} --token {token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OUqQ3TcLbXzA"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your CSV file\n",
        "file_path3 = f\"tweets-data/{filename3}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df3 = pd.read_csv(file_path3)\n",
        "\n",
        "# sort by created_at\n",
        "df3 = df3.sort_values(by=['created_at'])\n",
        "\n",
        "# change datetime to Asia/Jakarta\n",
        "df3['created_at'] = pd.to_datetime(df3['created_at'])\n",
        "df3['created_at'] = df3['created_at'].dt.tz_convert('Asia/Jakarta')\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPd3WWbbXzA"
      },
      "outputs": [],
      "source": [
        "df3.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fWoZgpDgsOp"
      },
      "source": [
        "## Review Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "try:\n",
        "    num_tweets = len(df)\n",
        "    print(f\"Jumlah tweet dalam dataframe 1 'Anies' adalah {num_tweets}.\")\n",
        "except:\n",
        "    pass\n",
        "try: \n",
        "    num_tweets2 = len(df2)\n",
        "    print(f\"Jumlah tweet dalam dataframe 3 'Muhaimin' adalah {num_tweets2}.\")\n",
        "except:\n",
        "    pass\n",
        "try: \n",
        "    num_tweets3 = len(df3)\n",
        "    print(f\"Jumlah tweet dalam dataframe 3 'Amin' adalah {num_tweets3}.\")\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export to csv\n",
        "path_drive = '/content/drive/MyDrive/ScrapingX/'\n",
        "try:\n",
        "    df.to_csv(f'{path_drive}{filename}', index=False)\n",
        "except:\n",
        "    print(f\"gagal menyimpan file: {filename}\")\n",
        "try:\n",
        "    df2.to_csv(f'{path_drive}{filename2}', index=False)\n",
        "except:\n",
        "    print(f\"gagal menyimpan file: {filename}\")\n",
        "try:\n",
        "    df3.to_csv(f'{path_drive}{filename3}', index=False)\n",
        "except:\n",
        "    print(f\"gagal menyimpan file: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcGkGVjx16-f"
      },
      "outputs": [],
      "source": [
        "# export to csv\n",
        "path_drive = '/content/drive/MyDrive/ScrapingX/'\n",
        "df.to_csv(f'{path_drive}{filename}', index=False)\n",
        "df2.to_csv(f'{path_drive}{filename2}', index=False)\n",
        "df3.to_csv(f'{path_drive}{filename3}', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zguWQpH1dbFm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('drive/MyDrive/ScrapingX'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
